"""
Section 4: Platform Readiness Assessment Agent
==============================================

This agent evaluates organizational readiness for adopting agentic AI
in platform engineering, aligned with "Effective Platform Engineering"
principles.
"""

import os
import json
import yaml
from typing import Dict, List, Tuple
from dataclasses import dataclass
from anthropic import Anthropic


@dataclass
class AssessmentScore:
    """Score for a specific dimension"""
    dimension: str
    current_score: int  # 1-5
    target_score: int   # 1-5
    gap: int
    priority: str  # 'high', 'medium', 'low'


class LLMProvider:
    """Multi-provider LLM support"""
    
    def __init__(self):
        self.provider = self._detect_provider()
        if self.provider == 'claude':
            self.client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
    
    def _detect_provider(self) -> str:
        if os.getenv('ANTHROPIC_API_KEY'):
            return 'claude'
        elif os.getenv('OPENAI_API_KEY'):
            return 'openai'
        else:
            print("âš ï¸  No API key found. Using mock mode.")
            return 'mock'
    
    def complete(self, prompt: str, max_tokens: int = 4000) -> str:
        if self.provider == 'claude':
            return self._claude_complete(prompt, max_tokens)
        else:
            return self._mock_complete()
    
    def _claude_complete(self, prompt: str, max_tokens: int) -> str:
        message = self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": prompt}]
        )
        return message.content[0].text
    
    def _mock_complete(self) -> str:
        return """
## Platform Readiness Assessment Report
**Organization:** Acme Corp  
**Date:** 2025-11-14  
**Readiness Score: 3.2/5.0** (Moderate - Ready with preparation)

---

### Executive Summary

Acme Corp demonstrates **moderate readiness** for adopting agentic AI in platform 
engineering. Strong automation foundation exists, but gaps in AI/ML expertise and 
observability maturity require attention before full-scale deployment.

**Recommendation:** Proceed with pilot implementation in Q1 2026  
**Timeline to Production:** 6-9 months  
**Total Investment:** $435K over 12 months  
**Expected Annual ROI:** 62% ($270K net benefit)

---

### Dimension Analysis

#### Platform Engineering Maturity: 4/5 âœ…
**Strengths:** Self-service platform, IaC at 85%, strong product thinking  
**Gaps:** Limited AI integration, manual incident response  
**Recommendation:** Maintain excellence while adding AI capabilities

#### Automation & Infrastructure: 4/5 âœ…
**Strengths:** Kubernetes 80%, GitOps implemented, automated testing  
**Gaps:** Limited self-healing, manual drift detection  
**Recommendation:** Add intelligent automation layer

#### Observability & Monitoring: 3/5 âš ï¸
**Strengths:** Centralized logging, good metric coverage  
**Gaps:** Limited tracing (30%), no log correlation, reactive monitoring  
**Critical:** Without comprehensive observability, agents cannot function effectively  
**Recommendation:** Implement OpenTelemetry, deploy log correlation (Priority: HIGH)

#### Team Capabilities: 3/5 âš ï¸
**Strengths:** 8 senior platform engineers, strong DevOps culture  
**Gaps:** Only 2/8 have AI/ML experience, no prompt engineering skills  
**Critical:** AI agent development requires different expertise  
**Recommendation:** Hire 2 ML engineers, train entire team (Priority: HIGH)

#### Incident Response: 4/5 âœ…
**Strengths:** MTTR 25min, 80% runbook coverage, strong postmortem culture  
**Gaps:** Manual correlation, limited prediction  
**Quick Win:** Ops intelligence agent can reduce MTTR by 40%

#### Security & Compliance: 4/5 âœ…
**Strengths:** SOC2 certified, automated scanning, good secret management  
**Gaps:** AI security understanding, no AI governance framework  
**Recommendation:** Develop AI-specific security controls

---

### Implementation Roadmap

#### Phase 1: Foundation (Months 1-3) - $200K

**Goals:** Build AI capabilities, enhance observability

**Key Activities:**
- Hire 2 ML engineers with platform background
- Train platform team on AI/ML fundamentals and prompt engineering
- Deploy OpenTelemetry across all services
- Implement log correlation platform
- Establish AI governance framework

**Success Metrics:**
- [ ] ML engineers onboarded
- [ ] 100% team trained on AI
- [ ] Distributed tracing >90%
- [ ] AI governance approved

---

#### Phase 2: Pilot (Months 4-6) - $40K

**Goals:** Deploy first agent, validate approach

**Key Activities:**
- Deploy diagnostic agent to non-production
- Collect user feedback and iterate
- Measure accuracy and value
- Define production rollout plan

**Pilot Scope:**
- 1 agent (infrastructure diagnostics)
- 10 users (platform team only)
- Non-production environments
- Manual approval required

**Success Criteria:**
- Agent accuracy >80%
- MTTR reduction >20%
- User satisfaction >4/5
- Zero security incidents

---

#### Phase 3: Expansion (Months 7-9) - $105K

**Goals:** Multiple agents in production

**Key Activities:**
- Deploy release readiness agent
- Deploy operational intelligence agent
- Gradual production rollout with monitoring
- Establish rollback procedures

**Success Metrics:**
- 3 agents operational
- 50+ active users
- MTTR reduction >40%
- Release cycle time -30%

---

#### Phase 4: Optimization (Months 10-12) - $90K

**Goals:** Optimize costs, scale capabilities

**Key Activities:**
- Implement caching and prompt optimization
- Add multi-agent orchestration
- Develop proactive recommendation engine
- Create internal best practices guide

**Success Metrics:**
- Cost per interaction <$0.50
- 80% incidents handled by agents
- 100% team adoption
- Positive ROI achieved

---

### Financial Analysis

**Total Investment:** $435K over 12 months  
**Expected Annual Benefits:** $705K
- MTTR improvement: $180K
- Faster releases: $150K  
- Reduced manual work: $200K
- Fewer incidents: $100K
- Quality improvements: $75K

**ROI: 62% first year**  
**Payback Period: ~7 months**

---

### Critical Success Factors

âœ… **Strong Foundation**
- Existing automation maturity
- Good platform engineering practices
- Supportive culture

âš ï¸ **Must Address**
- AI/ML expertise gap (HIGH priority)
- Observability gaps (HIGH priority)
- AI governance framework

ğŸš€ **Quick Wins Available**
- Ops intelligence agent (40% MTTR reduction)
- Diagnostic agent (60% faster root cause)
- Release agent (30% cycle time reduction)

---

### Alignment with "Effective Platform Engineering"

This roadmap embodies your book's core principles:

**Platform as a Product** âœ…
- Agents enhance self-service
- Focus on developer experience
- Continuous improvement

**Enabling Team Autonomy** âœ…
- 24/7 expertise access
- Reduced platform team dependency
- Faster self-service problem resolution

**Reducing Cognitive Load** âœ…
- Agents handle complex analysis
- Natural language interfaces
- Contextual recommendations

---

### Immediate Next Steps (30 Days)

1. **Approve Phase 1 budget** ($200K)
2. **Initiate ML engineer hiring** (2 positions)
3. **Schedule team training** (AI fundamentals + prompt engineering)
4. **Begin observability audit** (identify tracing gaps)

**Owner:** VP Engineering  
**Success Criteria:** Hiring in progress, training scheduled, audit complete

---

### Recommendation

**PROCEED with pilot implementation**

Acme Corp has a strong foundation and clear path to success. With targeted 
investments in AI expertise and observability, the organization can realize 
significant value from agentic AI within 6-9 months.

**Confidence Level: 85%** - High confidence based on strong fundamentals and 
clear mitigation strategies for identified gaps.
"""


class AssessmentEngine:
    """Core assessment logic"""
    
    DIMENSIONS = [
        "platform_maturity",
        "automation_infrastructure",
        "observability_monitoring",
        "team_capabilities",
        "incident_response",
        "security_compliance"
    ]
    
    def assess(self, assessment_data: Dict) -> List[AssessmentScore]:
        """Evaluate assessment and return scores"""
        scores = []
        
        for dimension in self.DIMENSIONS:
            dimension_data = assessment_data.get(dimension, {})
            current = dimension_data.get('score', 1)
            target = dimension_data.get('target', 4)
            
            gap = target - current
            priority = self._determine_priority(gap, dimension)
            
            scores.append(AssessmentScore(
                dimension=dimension,
                current_score=current,
                target_score=target,
                gap=gap,
                priority=priority
            ))
        
        return scores
    
    def _determine_priority(self, gap: int, dimension: str) -> str:
        """Determine priority based on gap and dimension"""
        critical_dimensions = ['observability_monitoring', 'team_capabilities']
        
        if gap >= 2:
            return 'high'
        elif gap == 1 and dimension in critical_dimensions:
            return 'high'
        elif gap == 1:
            return 'medium'
        else:
            return 'low'


class RoadmapGenerator:
    """Generates implementation roadmap"""
    
    def __init__(self):
        self.llm = LLMProvider()
    
    def generate(self, scores: List[AssessmentScore], 
                 assessment_data: Dict) -> str:
        """Generate comprehensive roadmap"""
        
        prompt = self._build_roadmap_prompt(scores, assessment_data)
        return self.llm.complete(prompt, max_tokens=4500)
    
    def _build_roadmap_prompt(self, scores: List[AssessmentScore],
                              assessment_data: Dict) -> str:
        """Build prompt for roadmap generation"""
        
        org_name = assessment_data.get('organization', {}).get('name', 'Organization')
        
        prompt = f"""You are an expert platform engineering consultant creating an 
implementation roadmap for adopting agentic AI in platform engineering.

## Organization Profile

{json.dumps(assessment_data.get('organization', {}), indent=2)}

## Assessment Scores

"""
        
        for score in scores:
            prompt += f"""
### {score.dimension.replace('_', ' ').title()}
- Current: {score.current_score}/5
- Target: {score.target_score}/5
- Gap: {score.gap}
- Priority: {score.priority.upper()}
"""
        
        prompt += """

## Platform Maturity Context

Current platform capabilities:
"""
        prompt += json.dumps(assessment_data.get('platform_maturity', {}), indent=2)
        
        prompt += """

## Team Context

Current team structure and capabilities:
"""
        prompt += json.dumps(assessment_data.get('team_capabilities', {}), indent=2)
        
        prompt += """

## Goals and Objectives

"""
        goals = assessment_data.get('goals', [])
        for goal in goals:
            prompt += f"- {goal}\n"
        
        prompt += """

## Required Roadmap Structure

Create a comprehensive assessment report with:

1. **Executive Summary** (3-4 sentences)
   - Overall readiness score (1-5)
   - Key recommendation (proceed/wait/prepare)
   - Timeline to production
   - Expected ROI summary

2. **Dimension Analysis** (for each of 6 dimensions)
   - Current state with specific metrics
   - Key strengths
   - Critical gaps
   - Specific recommendations
   - Priority level

3. **Implementation Roadmap** (4 phases over 12 months)
   
   **Phase 1: Foundation (Months 1-3)**
   - Specific goals
   - 3-5 key activities with timing
   - Success metrics (checkbox format)
   - Budget estimate
   
   **Phase 2: Pilot (Months 4-6)**
   - Pilot scope (agent, users, environment)
   - Success criteria
   - Risk mitigation
   - Budget estimate
   
   **Phase 3: Expansion (Months 7-9)**
   - Rollout strategy
   - Multiple agents deployment plan
   - Success metrics
   - Budget estimate
   
   **Phase 4: Optimization (Months 10-12)**
   - Optimization focus
   - Advanced capabilities
   - Long-term goals
   - Budget estimate

4. **Financial Analysis**
   - Total investment by phase
   - Expected annual benefits (quantified)
   - ROI calculation
   - Payback period

5. **Critical Success Factors**
   - Existing strengths to leverage
   - Must-address gaps
   - Quick wins available

6. **Alignment with "Effective Platform Engineering"**
   Reference these principles from the book:
   - Platform as a product
   - Enabling team autonomy
   - Reducing cognitive load
   - Establishing feedback loops

7. **Immediate Next Steps** (30 days)
   - 3-4 specific actions
   - Owners
   - Success criteria

8. **Final Recommendation**
   - Clear proceed/wait decision
   - Confidence level (%)
   - Key success factors

## Guidelines

- Be specific with numbers, timelines, and budgets
- Quantify expected benefits
- Be realistic about challenges
- Provide actionable recommendations
- Align with organization's specific context
- Reference "Effective Platform Engineering" principles
- Use clear Markdown formatting
- Include tables where helpful
- Use checkboxes for action items

Format as professional consultant report in Markdown.
"""
        
        return prompt


class ReadinessAgent:
    """Main platform readiness assessment agent"""
    
    def __init__(self):
        self.llm = LLMProvider()
        self.assessment_engine = AssessmentEngine()
        self.roadmap_generator = RoadmapGenerator()
    
    def assess(self, assessment_file: str) -> Tuple[str, List[AssessmentScore]]:
        """Complete readiness assessment"""
        
        print("ğŸ” Starting platform readiness assessment...")
        
        # Load assessment data
        print("ğŸ“¥ Loading assessment data...")
        with open(assessment_file, 'r') as f:
            assessment_data = yaml.safe_load(f)
        
        org_name = assessment_data.get('organization', {}).get('name', 'Organization')
        print(f"   Organization: {org_name}")
        
        # Run assessment
        print("\nğŸ“Š Evaluating maturity scores...")
        scores = self.assessment_engine.assess(assessment_data)
        
        # Print summary
        self._print_score_summary(scores)
        
        # Generate roadmap
        print("\nğŸ—ºï¸  Generating implementation roadmap...")
        roadmap = self.roadmap_generator.generate(scores, assessment_data)
        
        print("\nâœ… Assessment complete!")
        return roadmap, scores
    
    def _print_score_summary(self, scores: List[AssessmentScore]):
        """Print assessment score summary"""
        print("\n" + "="*70)
        print("ASSESSMENT SUMMARY")
        print("="*70)
        
        for score in scores:
            status = "âœ…" if score.gap == 0 else "âš ï¸" if score.gap == 1 else "ğŸš¨"
            priority = score.priority.upper()
            
            print(f"\n{status} {score.dimension.replace('_', ' ').title()}")
            print(f"   Current: {score.current_score}/5")
            print(f"   Target: {score.target_score}/5")
            print(f"   Gap: {score.gap}")
            print(f"   Priority: {priority}")
        
        # Overall score
        avg_current = sum(s.current_score for s in scores) / len(scores)
        avg_target = sum(s.target_score for s in scores) / len(scores)
        
        print("\n" + "="*70)
        print(f"Overall Current Maturity: {avg_current:.1f}/5.0")
        print(f"Target Maturity: {avg_target:.1f}/5.0")
        print(f"Average Gap: {avg_target - avg_current:.1f}")


# Command-line interface
if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Platform Readiness Assessment Agent'
    )
    parser.add_argument(
        '--assessment',
        required=True,
        help='Assessment YAML file'
    )
    parser.add_argument(
        '--output',
        default='readiness-report.md',
        help='Output file for assessment report'
    )
    
    args = parser.parse_args()
    
    # Run assessment
    agent = ReadinessAgent()
    roadmap, scores = agent.assess(args.assessment)
    
    # Save report
    with open(args.output, 'w') as f:
        f.write(roadmap)
    
    print(f"\nğŸ“„ Assessment report saved to: {args.output}")
