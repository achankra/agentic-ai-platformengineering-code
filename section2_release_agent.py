"""
Section 2: Release Readiness Agent
===================================

This agent evaluates deployment readiness by analyzing quality gates:
- Test coverage
- Performance metrics
- Security vulnerabilities
- Code complexity
"""

import os
import json
import subprocess
from typing import Dict, List, Tuple
from dataclasses import dataclass
from anthropic import Anthropic


@dataclass
class QualityGate:
    """Quality gate definition"""
    name: str
    threshold: float
    actual_value: float
    unit: str
    passed: bool
    severity: str  # 'critical', 'high', 'medium', 'low'


class LLMProvider:
    """Multi-provider LLM support"""
    
    def __init__(self):
        self.provider = self._detect_provider()
        if self.provider == 'claude':
            self.client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
    
    def _detect_provider(self) -> str:
        if os.getenv('ANTHROPIC_API_KEY'):
            return 'claude'
        elif os.getenv('OPENAI_API_KEY'):
            return 'openai'
        else:
            print("‚ö†Ô∏è  No API key found. Using mock mode.")
            return 'mock'
    
    def complete(self, prompt: str, max_tokens: int = 2000) -> str:
        if self.provider == 'claude':
            return self._claude_complete(prompt, max_tokens)
        else:
            return self._mock_complete()
    
    def _claude_complete(self, prompt: str, max_tokens: int) -> str:
        message = self.client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=max_tokens,
            messages=[{"role": "user", "content": prompt}]
        )
        return message.content[0].text
    
    def _mock_complete(self) -> str:
        return """
## Release Decision: APPROVE WITH MONITORING

### Overall Assessment
This release meets all critical quality gates and can proceed to production. 
However, the performance metrics warrant close monitoring during rollout.

### Quality Gates Summary

‚úÖ **Test Coverage: PASS**
- Actual: 82.5% line coverage
- Threshold: ‚â•80%
- Status: Exceeds minimum by 2.5%
- Trend: +3% from last release

‚úÖ **Performance: PASS (Marginal)**
- P95 Latency: +8.2% increase
- Threshold: ‚â§10% increase
- Status: Within acceptable range but approaching limit
- Recommendation: Monitor closely, investigate if trends upward

‚úÖ **Security: PASS**
- Critical vulnerabilities: 0
- High severity: 3 (threshold: <5)
- Medium severity: 7
- Status: Acceptable for release
- Action: Plan remediation for high-severity issues in next sprint

‚úÖ **Complexity: PASS**
- Average cyclomatic complexity: 7.8
- Threshold: <10
- Status: Well within acceptable range
- Quality: Code maintainability is good

### Detailed Analysis

**Test Coverage Strength:**
The 82.5% coverage represents solid test discipline. Critical paths are well-covered.
The 3% improvement from last release shows positive trend.

**Performance Consideration:**
The 8.2% latency increase is notable. Analysis suggests:
- New features added complexity to hot paths
- Database query optimization may help
- Consider implementing caching layer
- Set up canary deployment with automatic rollback at 9% threshold

**Security Posture:**
Zero critical vulnerabilities is excellent. The 3 high-severity issues are:
1. Outdated dependency in dev environment (low runtime risk)
2. Missing input validation in admin endpoint (limited exposure)
3. Weak cipher suite in legacy API (deprecated endpoint)

None are blocking, but should be prioritized for next cycle.

**Code Quality:**
Complexity score of 7.8 indicates maintainable code. No concerning hotspots detected.

### Deployment Recommendations

**Rollout Strategy:**
1. Deploy to canary environment (5% traffic)
2. Monitor P95 latency closely for 2 hours
3. If latency remains stable, proceed to 25% traffic
4. Full rollout after 4 hours of stability

**Automated Rollback Triggers:**
- P95 latency exceeds 10% increase
- Error rate exceeds 0.5%
- Any critical security alert

**Monitoring Checklist:**
- [ ] P95 latency tracking every 5 minutes
- [ ] Error rate dashboard active
- [ ] Database connection pool metrics
- [ ] Memory usage trending
- [ ] On-call engineer notified

### Confidence Score: 85/100

**Decision: PROCEED TO PRODUCTION**

Rationale: All quality gates passed. Performance metrics require attention but are 
within acceptable parameters. Recommend enhanced monitoring during rollout with 
clear rollback criteria established.
"""


class CoverageAnalyzer:
    """Analyzes test coverage"""
    
    def analyze(self, project_dir: str) -> Dict:
        """Run coverage analysis"""
        try:
            # Run pytest with coverage
            result = subprocess.run(
                ['pytest', '--cov=src', '--cov-report=json', project_dir],
                capture_output=True,
                text=True,
                timeout=120
            )
            
            # Read coverage report
            try:
                with open('coverage.json', 'r') as f:
                    coverage_data = json.load(f)
                    return {
                        'line_coverage': coverage_data['totals']['percent_covered'],
                        'branch_coverage': coverage_data['totals'].get('percent_covered_display', 0),
                        'files': len(coverage_data['files']),
                        'lines_covered': coverage_data['totals']['covered_lines'],
                        'lines_total': coverage_data['totals']['num_statements']
                    }
            except FileNotFoundError:
                # Fallback mock data
                return {
                    'line_coverage': 82.5,
                    'branch_coverage': 75.3,
                    'files': 15,
                    'lines_covered': 850,
                    'lines_total': 1030
                }
        except Exception as e:
            print(f"‚ö†Ô∏è  Coverage analysis error: {e}")
            return {'line_coverage': 82.5}  # Mock fallback


class PerformanceAnalyzer:
    """Analyzes performance metrics"""
    
    def analyze(self, test_results_file: str) -> Dict:
        """Extract performance metrics"""
        try:
            with open(test_results_file, 'r') as f:
                results = json.load(f)
            
            return {
                'p50_latency_ms': results.get('p50', 45),
                'p95_latency_ms': results.get('p95', 125),
                'p99_latency_ms': results.get('p99', 250),
                'baseline_p95_ms': 115,  # Previous release
                'throughput_rps': results.get('throughput', 1500)
            }
        except Exception:
            # Mock data
            return {
                'p50_latency_ms': 45,
                'p95_latency_ms': 125,
                'p99_latency_ms': 250,
                'baseline_p95_ms': 115,
                'throughput_rps': 1500
            }


class SecurityAnalyzer:
    """Analyzes security vulnerabilities"""
    
    def analyze(self, project_dir: str) -> Dict:
        """Run security scans"""
        try:
            # Try running safety or snyk
            result = subprocess.run(
                ['safety', 'check', '--json'],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.stdout:
                data = json.loads(result.stdout)
                return self._categorize_vulnerabilities(data)
            else:
                return self._mock_vulnerabilities()
                
        except Exception:
            return self._mock_vulnerabilities()
    
    def _mock_vulnerabilities(self) -> Dict:
        """Provide mock vulnerability data"""
        return {
            'critical': 0,
            'high': 3,
            'medium': 7,
            'low': 12,
            'vulnerabilities': [
                {
                    'severity': 'high',
                    'package': 'requests',
                    'description': 'Outdated version in dev dependencies'
                },
                {
                    'severity': 'high',
                    'package': 'admin-api',
                    'description': 'Missing input validation'
                },
                {
                    'severity': 'high',
                    'package': 'legacy-crypto',
                    'description': 'Weak cipher suite'
                }
            ]
        }
    
    def _categorize_vulnerabilities(self, data: Dict) -> Dict:
        """Categorize vulnerabilities by severity"""
        categories = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        vulnerabilities = []
        
        for vuln in data.get('vulnerabilities', []):
            severity = vuln.get('severity', 'low').lower()
            categories[severity] = categories.get(severity, 0) + 1
            vulnerabilities.append(vuln)
        
        return {**categories, 'vulnerabilities': vulnerabilities}


class ComplexityAnalyzer:
    """Analyzes code complexity"""
    
    def analyze(self, project_dir: str) -> Dict:
        """Calculate cyclomatic complexity"""
        try:
            result = subprocess.run(
                ['radon', 'cc', project_dir, '-a', '--json'],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.stdout:
                data = json.load(result.stdout)
                return self._process_complexity(data)
            else:
                return self._mock_complexity()
                
        except Exception:
            return self._mock_complexity()
    
    def _mock_complexity(self) -> Dict:
        """Provide mock complexity data"""
        return {
            'average_complexity': 7.8,
            'max_complexity': 15,
            'files_analyzed': 15,
            'functions_high_complexity': 2
        }


class ReleaseEvaluator:
    """Main release readiness evaluation agent"""
    
    def __init__(self):
        self.llm = LLMProvider()
        self.coverage = CoverageAnalyzer()
        self.performance = PerformanceAnalyzer()
        self.security = SecurityAnalyzer()
        self.complexity = ComplexityAnalyzer()
    
    def evaluate(self, config_file: str = 'quality-gates.yml') -> Tuple[str, List[QualityGate]]:
        """Evaluate release readiness"""
        
        print("üéØ Starting release readiness evaluation...")
        
        # Load quality gate thresholds
        thresholds = self._load_thresholds(config_file)
        
        # Run all analyses
        print("üìä Analyzing test coverage...")
        coverage_data = self.coverage.analyze('./src')
        
        print("‚ö° Analyzing performance...")
        performance_data = self.performance.analyze('./test-results.json')
        
        print("üîí Analyzing security...")
        security_data = self.security.analyze('./src')
        
        print("üìà Analyzing complexity...")
        complexity_data = self.complexity.analyze('./src')
        
        # Evaluate quality gates
        gates = self._evaluate_gates(
            thresholds, coverage_data, performance_data,
            security_data, complexity_data
        )
        
        # Generate AI decision
        print("\nü§ñ Generating release decision...")
        prompt = self._build_evaluation_prompt(gates, coverage_data,
                                                performance_data, security_data,
                                                complexity_data)
        
        decision = self.llm.complete(prompt, max_tokens=3000)
        
        print("\n‚úÖ Evaluation complete!")
        return decision, gates
    
    def _load_thresholds(self, config_file: str) -> Dict:
        """Load quality gate thresholds from config"""
        # Default thresholds
        return {
            'coverage_min': 80.0,
            'performance_p95_increase_max': 10.0,
            'security_critical_max': 0,
            'security_high_max': 5,
            'complexity_max': 10.0
        }
    
    def _evaluate_gates(self, thresholds: Dict, coverage: Dict,
                       performance: Dict, security: Dict,
                       complexity: Dict) -> List[QualityGate]:
        """Evaluate all quality gates"""
        
        gates = []
        
        # Coverage gate
        coverage_pct = coverage.get('line_coverage', 0)
        gates.append(QualityGate(
            name="Test Coverage",
            threshold=thresholds['coverage_min'],
            actual_value=coverage_pct,
            unit="%",
            passed=coverage_pct >= thresholds['coverage_min'],
            severity='critical'
        ))
        
        # Performance gate
        current_p95 = performance.get('p95_latency_ms', 0)
        baseline_p95 = performance.get('baseline_p95_ms', 100)
        p95_increase = ((current_p95 - baseline_p95) / baseline_p95) * 100
        
        gates.append(QualityGate(
            name="Performance",
            threshold=thresholds['performance_p95_increase_max'],
            actual_value=p95_increase,
            unit="% increase",
            passed=p95_increase <= thresholds['performance_p95_increase_max'],
            severity='high'
        ))
        
        # Security gates
        gates.append(QualityGate(
            name="Security (Critical)",
            threshold=thresholds['security_critical_max'],
            actual_value=security.get('critical', 0),
            unit="vulnerabilities",
            passed=security.get('critical', 0) <= thresholds['security_critical_max'],
            severity='critical'
        ))
        
        gates.append(QualityGate(
            name="Security (High)",
            threshold=thresholds['security_high_max'],
            actual_value=security.get('high', 0),
            unit="vulnerabilities",
            passed=security.get('high', 0) < thresholds['security_high_max'],
            severity='high'
        ))
        
        # Complexity gate
        avg_complexity = complexity.get('average_complexity', 0)
        gates.append(QualityGate(
            name="Code Complexity",
            threshold=thresholds['complexity_max'],
            actual_value=avg_complexity,
            unit="cyclomatic complexity",
            passed=avg_complexity < thresholds['complexity_max'],
            severity='medium'
        ))
        
        return gates
    
    def _build_evaluation_prompt(self, gates: List[QualityGate],
                                 coverage: Dict, performance: Dict,
                                 security: Dict, complexity: Dict) -> str:
        """Build evaluation prompt for LLM"""
        
        prompt = """You are a senior platform engineer making a release decision.

Analyze the following quality gate results and provide a comprehensive release decision.

## Quality Gates

"""
        for gate in gates:
            status = "‚úÖ PASS" if gate.passed else "‚ùå FAIL"
            prompt += f"""
### {gate.name} ({gate.severity.upper()})
- Status: {status}
- Threshold: {gate.threshold} {gate.unit}
- Actual: {gate.actual_value:.2f} {gate.unit}
"""
        
        prompt += f"""

## Detailed Metrics

### Coverage
```json
{json.dumps(coverage, indent=2)}
```

### Performance
```json
{json.dumps(performance, indent=2)}
```

### Security
```json
{json.dumps(security, indent=2)}
```

### Complexity
```json
{json.dumps(complexity, indent=2)}
```

## Required Decision Format

Provide:
1. **Release Decision**: APPROVE / APPROVE WITH MONITORING / REJECT
2. **Overall Assessment**: 2-3 sentence summary
3. **Quality Gates Summary**: Brief status of each gate
4. **Detailed Analysis**: Deep dive into concerning metrics
5. **Deployment Recommendations**: Rollout strategy, monitoring, rollback triggers
6. **Confidence Score**: 0-100

Be specific and actionable. Consider the severity of each gate when making the decision.
"""
        
        return prompt
    
    def print_summary(self, gates: List[QualityGate]):
        """Print quality gates summary"""
        print("\n" + "="*70)
        print("QUALITY GATES SUMMARY")
        print("="*70)
        
        for gate in gates:
            status = "‚úÖ PASS" if gate.passed else "‚ùå FAIL"
            print(f"\n{status} {gate.name}")
            print(f"   Threshold: {gate.threshold} {gate.unit}")
            print(f"   Actual: {gate.actual_value:.2f} {gate.unit}")
            print(f"   Severity: {gate.severity.upper()}")
        
        all_passed = all(g.passed for g in gates)
        critical_passed = all(g.passed for g in gates if g.severity == 'critical')
        
        print("\n" + "="*70)
        if all_passed:
            print("üéâ All quality gates PASSED")
        elif critical_passed:
            print("‚ö†Ô∏è  Critical gates passed, some non-critical gates failed")
        else:
            print("üö´ Critical quality gates FAILED - Do not deploy")


# Command-line interface
if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Release Readiness Evaluation Agent'
    )
    parser.add_argument(
        '--config',
        default='quality-gates.yml',
        help='Quality gates configuration file'
    )
    parser.add_argument(
        '--output',
        default='release-decision.md',
        help='Output file for release decision'
    )
    
    args = parser.parse_args()
    
    # Run evaluation
    evaluator = ReleaseEvaluator()
    decision, gates = evaluator.evaluate(args.config)
    
    # Print summary
    evaluator.print_summary(gates)
    
    # Save decision
    with open(args.output, 'w') as f:
        f.write(decision)
    
    print(f"\nüìÑ Release decision saved to: {args.output}")
